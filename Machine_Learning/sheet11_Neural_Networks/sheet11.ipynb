{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes\n",
    "\n",
    "In this exercise, you will implement Gaussian process regression and apply it to a toy and a real dataset. We use the notation used in the paper \"Rasmussen (2005). Gaussian Processes in Machine Learning\" linked on ISIS.\n",
    "\n",
    "Let us first draw a training set $X = (x_1,\\dots,x_n)$ and a test set $X_\\star = (x^\\star_1,\\dots,x^\\star_m)$ from a $d$-dimensional input distribution. The Gaussian Process is a model under which the real-valued outputs $\\mathbf{f} = (f_1,\\dots,f_n)$ and $\\mathbf{f}_\\star = (f^\\star_1,\\dots,f^\\star_m)$ associated to $X$ and $X_\\star$ follow the Gaussian distribution:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left[\n",
    "\\begin{array}{c}\\mathbf{f}\\\\\n",
    "\\mathbf{f}_\\star\\end{array}\n",
    "\\right]\n",
    "\\sim\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\boldsymbol{0}\\\\\n",
    "\\boldsymbol{0}\n",
    "\\end{array}\n",
    "\\right]\n",
    ",\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\Sigma & \\Sigma_\\star\\\\\n",
    "\\Sigma_\\star^\\top & \\Sigma_{\\star\\star}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\Sigma &= k(X,X)+\\sigma^2 I\\\\\n",
    "\\Sigma_\\star &= k(X,X_\\star)\\\\\n",
    "\\Sigma_{\\star\\star} &= k(X_\\star,X_\\star)+\\sigma^2 I\n",
    "\\end{align*}\n",
    "\n",
    "and where $k(\\cdot,\\cdot)$ is the Gaussian kernel function. (The kernel function is implemented in `utils.py`.) Predicting the output for new data points $X_\\star$ is achieved by conditioning the joint probability distribution on the training set. Such conditional distribution called posterior distribution can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{f}_\\star | \\mathbf{f} \\sim \\mathcal{N} (\n",
    "\\underbrace{\\Sigma_\\star^\\top \\Sigma^{-1} \\mathbf{f}}_{\\boldsymbol{\\mu}_\\star}\n",
    "~,~\n",
    "\\underbrace{\\Sigma_{\\star\\star} - \\Sigma_\\star^\\top \\Sigma^{-1} \\Sigma_\\star}_{C_\\star}\n",
    ")\n",
    "\\end{equation}\n",
    "\n",
    "Having inferred the posterior distribution, the log-likelihood of observing for the inputs $X_\\star$ the outputs $\\mathbf{y}_\\star$ is given by evaluating the distribution $\\mathbf{f}_\\star | \\mathbf{f}$ at $\\mathbf{y}_\\star$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(\\mathbf{y}_\\star | \\mathbf{f}) = -\\frac1{2} (\\mathbf{y}_\\star - \\boldsymbol{\\mu}_\\star)^\\top C^{-1}_\\star (\\mathbf{y}_\\star - \\boldsymbol{\\mu}_\\star) - \\frac1{2}\\log|C_\\star| - \\frac{m}{2}\\log2\\pi\n",
    "\\end{equation}\n",
    "\n",
    "where $|\\cdot|$ is the determinant. Note that the likelihood of the data given this posterior distribution can be measured both for the training data and the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing a Gaussian Process (20 P)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* **Create a class `GP_Regressor` that implements a Gaussian process regressor and has the following three methods:**\n",
    "\n",
    " * **`def __init__(self,Xtrain,Ytrain,width,noise):`** Initialize a Gaussian process with noise parameter $\\sigma$ and width parameter $w$. The function must also precompute the matrix $\\Sigma^{-1}$ for subsequent use by the method `predict()` and `loglikelihood()`.\n",
    "\n",
    " * **`def predict(self,Xtest):`** For the test set $X_\\star$ of $m$ points received as parameter, return the mean vector of size $m$ and covariance matrix of size $m \\times m$ of the corresponding output, that is, return the parameters $(\\boldsymbol{\\mu}_\\star,C_\\star)$ of the Gaussian distribution $\\mathbf{f}_\\star | \\mathbf{f}$.\n",
    "\n",
    " * **`def loglikelihood(self,Xtest,Ytest):`** For a data set $X_\\star$ of $m$ test points received as first parameter, return the loglikelihood of observing the outputs $\\mathbf{y}_\\star$ received as second parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------\n",
    "# TODO: Replace by your code\n",
    "# --------------------------\n",
    "\"\"\"\n",
    "https://towardsdatascience.com/understanding-gaussian-process-the-socratic-way-ba02369d804\n",
    "X is the set of locations that we have in our training data. X is a vector of length n\n",
    "\n",
    "X_* the set of test locations where we want to evaluate our underlying function f. \n",
    "Remember the goal of this regression task is to find f so we can get its values at test locations.\n",
    "\n",
    "We need a way to describe the dependency relationships among random variables. \n",
    "In Gaussian Process, we use the multivariate Gaussian distribution over these random variables \n",
    "to describe the means of those random variables and how they are correlated. \n",
    "A multivariate Gaussian distribution is specified by a mean vector and a covariance matrix\n",
    "\n",
    "The function k appearing in the covariance matrix. We call k a kernel function. \n",
    "We use k to define the covariance between every two random variables f(x) and f(x′) at location x and x′. \n",
    "\"\"\"\n",
    "class GP_Regressor():\n",
    "    def __init__(self,Xtrain,Ytrain,width,noise):\n",
    "        # Kernel of the observations\n",
    "        self.X = Xtrain\n",
    "        self.Y = Ytrain\n",
    "        self.width = width\n",
    "        self.noise = noise\n",
    "        self.kernel = utils.gaussianKernel(Xtrain, Xtrain, width)\n",
    "        self.kernel_inv = numpy.linalg.inv(self.kernel)\n",
    "        \n",
    "    def predict(self,Xtest):\n",
    "        self.kernel_pred = utils.gaussianKernel(self.X, Xtest, self.width)\n",
    "        mean = np.mean(Xtest, axis=1)\n",
    "        cov = np.cov(Xtest)\n",
    "        return (mean, cov)\n",
    "        \n",
    "    def loglikelihood(self,Xtest,Ytest):\n",
    "        pass\n",
    "    \n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test your implementation by running the code below (it visualizes the mean and variance of the prediction at every location of the input space) and compares the behavior of the Gaussian process for various noise parameters $\\sigma$ and width parameters $w$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8b4fa0a5a505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Plot the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'noise=%.1f width=%.1f lltrain=%.1f, lltest=%.1f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlltrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlltest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import utils,datasets,numpy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Open the toy data\n",
    "Xtrain,Ytrain,Xtest,Ytest = utils.split(*datasets.toy())\n",
    "\n",
    "# Create an analysis distribution\n",
    "Xrange = numpy.arange(-3.5,3.51,0.025)[:,numpy.newaxis]\n",
    "\n",
    "f = plt.figure(figsize=(18,15))\n",
    "\n",
    "# Loop over several parameters:\n",
    "for i,noise in enumerate([2.5,0.5,0.1]):\n",
    "    for j,width in enumerate([0.1,0.5,2.5]):\n",
    "\n",
    "        # Create Gaussian process regressor object\n",
    "        gp = GP_Regressor(Xtrain,Ytrain,width,noise)\n",
    "        \n",
    "        # Compute the predicted mean and variance for test data\n",
    "        mean,cov = gp.predict(Xrange)\n",
    "        var = cov.diagonal()\n",
    "        \n",
    "        # Compute the log-likelihood of training and test data\n",
    "        lltrain = gp.loglikelihood(Xtrain,Ytrain)\n",
    "        lltest  = gp.loglikelihood(Xtest ,Ytest )\n",
    "     \n",
    "        # Plot the data\n",
    "        p = f.add_subplot(3,3,3*i+j+1)\n",
    "        p.set_title('noise=%.1f width=%.1f lltrain=%.1f, lltest=%.1f'%(noise,width,lltrain,lltest))\n",
    "        p.set_xlabel('x')\n",
    "        p.set_ylabel('y')\n",
    "        p.scatter(Xtrain,Ytrain,color='green',marker='x') # training data\n",
    "        p.scatter(Xtest,Ytest,color='green',marker='o')   # test data\n",
    "        p.plot(Xrange,mean,color='blue')                  # GP mean\n",
    "        p.plot(Xrange,mean+var**.5,color='red')           # GP mean + std\n",
    "        p.plot(Xrange,mean-var**.5,color='red')           # GP mean - std\n",
    "        p.set_xlim(-3.5,3.5)\n",
    "        p.set_ylim(-4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 2: Application to the Yacht Hydrodynamics Data Set (10 P)\n",
    "\n",
    "In the second part, we would like to apply the Gaussian process regressor that you have implemented to a real dataset: the Yacht Hydrodynamics Data Set available on the UCI repository at the webpage http://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics. As stated on the web page, the input variables for this regression problem are:\n",
    "\n",
    "1. Longitudinal position of the center of buoyancy\n",
    "2. Prismatic coefficient\n",
    "3. Length-displacement ratio\n",
    "4. Beam-draught ratio\n",
    "5. Length-beam ratio\n",
    "6. Froude number\n",
    "\n",
    "and we would like to predict from these variables the residuary resistance per unit weight of displacement (last column in the file `yacht_hydrodynamics.data`).\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* **Load the data using `datasets.yacht()` and partition the data between training and test set using the function `utils.split()`. Normalize the data (center and rescale) so that the training data and labels have mean 0 and standard deviation 1 over the dataset for each dimension.**\n",
    "\n",
    "\n",
    "* **Train several Gaussian processes on the regression task using various width and noise parameters.**\n",
    "\n",
    "\n",
    "* **Draw two contour plots where the training and test log-likelihood are plotted as a function of the noise and width parameters. Choose suitable ranges of parameters so that the best parameter combination for the test set is in the plot. Use the same ranges and contour levels for training and test plots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# TODO: Replace by your code\n",
    "# --------------------------\n",
    "import solutions\n",
    "%matplotlib inline\n",
    "solutions.yacht()\n",
    "# --------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
